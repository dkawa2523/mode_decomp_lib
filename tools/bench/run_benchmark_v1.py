#!/usr/bin/env python3
from __future__ import annotations

import argparse
import csv
import os
from pathlib import Path
import sys
from typing import Any

import yaml

PROJECT_ROOT = Path(__file__).resolve().parents[2]

# Bench leaderboards can contain large stringified arrays (e.g. energy_cumsum).
# Increase the CSV field size limit so we can read them on all platforms.
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

from tools.bench._util import (  # noqa: E402
    raise_csv_field_size_limit,
    read_csv as _read_csv,
    to_float as _to_float,
    to_int as _to_int,
    write_csv as _write_csv,
)

raise_csv_field_size_limit(mib=50)


def _pareto_front(rows: list[dict[str, Any]], *, x_key: str, y_key: str) -> list[dict[str, Any]]:
    """Return non-dominated rows minimizing (x,y)."""
    pts: list[tuple[float, float, dict[str, Any]]] = []
    for r in rows:
        x = _to_float(r.get(x_key))
        y = _to_float(r.get(y_key))
        if x is None or y is None:
            continue
        pts.append((x, y, r))
    pts.sort(key=lambda t: (t[0], t[1]))
    out: list[dict[str, Any]] = []
    best_y = float("inf")
    for _x, y, r in pts:
        if y < best_y - 1e-15:
            out.append(r)
            best_y = y
    return out


def _link_if_exists(path: Path, *, label: str, base_dir: Path) -> str | None:
    if not path.exists():
        return None
    try:
        rel = os.path.relpath(path, start=base_dir)
        return f"[{label}]({Path(rel).as_posix()})"
    except Exception:
        return f"[{label}]({path.as_posix()})"


def _collect_decomp_plot_links(row: dict[str, Any], *, base_dir: Path) -> list[str]:
    run_dir = row.get("decomposition_run_dir")
    if not run_dir:
        return []
    root = Path(str(run_dir))
    candidates = [
        ("mode_r2_vs_k", root / "plots" / "mode_r2_vs_k.png"),
        ("scatter", root / "plots" / "field_scatter_true_vs_recon_ch0.png"),
        ("per_pixel_r2_map", root / "plots" / "per_pixel_r2_map_ch0.png"),
        ("coeff_mode_hist", root / "plots" / "coeff_mode_hist.png"),
        ("coeff_spectrum", root / "plots" / "coeff_spectrum.png"),
        ("coeff_hist", root / "plots" / "coeff_hist.png"),
        ("mask_fraction_hist", root / "plots" / "mask_fraction_hist.png"),
    ]
    out: list[str] = []
    for label, path in candidates:
        link = _link_if_exists(path, label=label, base_dir=base_dir)
        if link:
            out.append(link)
    # Always include plots dir link if it exists.
    plots_dir = root / "plots"
    link = _link_if_exists(plots_dir, label="plots_dir", base_dir=base_dir)
    if link:
        out.append(link)
    return out


def _collect_train_plot_links(row: dict[str, Any], *, base_dir: Path) -> list[str]:
    run_dir = row.get("train_run_dir")
    if not run_dir:
        return []
    root = Path(str(run_dir))
    candidates = [
        ("val_scatter_dim0", root / "plots" / "val_scatter_dim_0000.png"),
        ("val_residual_hist", root / "plots" / "val_residual_hist.png"),
        ("field_scatter", root / "plots" / "field_eval" / "field_scatter_true_vs_pred_ch0.png"),
        ("per_pixel_r2_map", root / "plots" / "field_eval" / "per_pixel_r2_map_ch0.png"),
        ("mode_r2_vs_k", root / "plots" / "field_eval" / "mode_r2_vs_k.png"),
    ]
    out: list[str] = []
    for label, path in candidates:
        link = _link_if_exists(path, label=label, base_dir=base_dir)
        if link:
            out.append(link)
    plots_dir = root / "plots"
    link = _link_if_exists(plots_dir, label="plots_dir", base_dir=base_dir)
    if link:
        out.append(link)
    return out


def _write_axis_summary_md(
    path: Path,
    *,
    decomp_rows: list[dict[str, Any]],
    train_rows: list[dict[str, Any]],
    cases: list[str],
) -> None:
    base_dir = path.parent  # runs/.../summary
    lines: list[str] = [
        "# Benchmark Axes Summary (v1)",
        "",
        "This file is generated by `tools/bench/run_benchmark_v1.py`.",
        "",
        "## Notes On Metrics (Important)",
        "- `field_r2` is the R^2 of the *full* reconstruction using all coefficients produced by the decomposer. For invertible transforms (e.g. DCT/FFT) this is typically ~1.0 and does **not** mean `K=1` is enough.",
        "- `n_components_required` (`n_req`) is derived from `energy_cumsum` (energy threshold, default 0.9). The *ordering* of `energy_cumsum` depends on the method (e.g. `fft2` uses frequency-radius ordering so negative frequencies are counted early). For offset-dominant datasets, energy concentrates in the DC/low-order terms, so `n_req` can be small even when fine structure needs more modes.",
        "- For fixed-K behavior, use `mode_r2_vs_k` plots and/or `field_r2_k*` metrics (when available).",
        "",
        "## Plot Links (What To Look At)",
        "- `mode_r2_vs_k`: reconstruction R^2 vs number of modes (CK-like coefficients only)",
        "- `scatter`: true vs reconstructed (or predicted) scatter with R^2 in title",
        "- `per_pixel_r2_map`: per-pixel R^2 heatmap across samples",
        "- `coeff_spectrum` / `coeff_hist`: coefficient energy/hist diagnostics",
        "- `mask_fraction_hist`: mask density diagnostics (mask domains)",
        "",
    ]

    lines += ["## Decomposition", ""]
    for case in cases:
        rows = [r for r in decomp_rows if r.get("case") == case]
        ok = [r for r in rows if r.get("status") == "ok"]
        lines.append(f"### {case}")
        lines.append(f"- methods: {len(ok)}/{len(rows)} ok")

        def _best_by(key: str) -> dict[str, Any] | None:
            scored = [(_to_float(r.get(key)), r) for r in ok]
            scored = [(v, r) for (v, r) in scored if v is not None]
            if not scored:
                return None
            scored.sort(key=lambda t: t[0])
            return scored[0][1]

        best_rmse = _best_by("field_rmse")
        fastest = _best_by("fit_time_sec")
        fewest = None
        scored = [(_to_int(r.get("n_components_required")), r) for r in ok]
        scored = [(v, r) for (v, r) in scored if v is not None]
        if scored:
            scored.sort(key=lambda t: t[0])
            fewest = scored[0][1]

        def _fmt(r: dict[str, Any] | None) -> str:
            if not r:
                return "n/a"
            r2k_parts = []
            for k in (1, 4, 16, 64):
                v = r.get(f"field_r2_k{k}")
                if v not in (None, ""):
                    r2k_parts.append(f"k{k}={v}")
            r2k = (" " + " ".join(r2k_parts)) if r2k_parts else ""
            return (
                f"{r.get('decompose')} rmse={r.get('field_rmse')} r2={r.get('field_r2')} time={r.get('fit_time_sec')} "
                f"n_req={r.get('n_components_required')}{r2k}"
            )

        lines.append(f"- best_rmse: {_fmt(best_rmse)}")
        lines.append(f"- fastest_fit: {_fmt(fastest)}")
        lines.append(f"- fewest_components: {_fmt(fewest)}")
        pareto = _pareto_front(ok, x_key="field_rmse", y_key="fit_time_sec")
        if pareto:
            lines.append("- pareto(rmse,time): " + ", ".join(str(r.get("decompose")) for r in pareto[:8]))
        if best_rmse:
            links = _collect_decomp_plot_links(best_rmse, base_dir=base_dir)
            if links:
                lines.append("- plots(best_rmse): " + ", ".join(links))
        lines.append("")

    lines += ["## Train", ""]
    for case in cases:
        rows = [r for r in train_rows if r.get("case") == case]
        ok = [r for r in rows if r.get("status") == "ok"]
        lines.append(f"### {case}")
        lines.append(f"- runs: {len(ok)}/{len(rows)} ok")

        best_val = None
        scored = [(_to_float(r.get("val_rmse")), r) for r in ok]
        scored = [(v, r) for (v, r) in scored if v is not None]
        if scored:
            scored.sort(key=lambda t: t[0])
            best_val = scored[0][1]

        fastest = None
        scored = [(_to_float(r.get("fit_time_sec")), r) for r in ok]
        scored = [(v, r) for (v, r) in scored if v is not None]
        if scored:
            scored.sort(key=lambda t: t[0])
            fastest = scored[0][1]

        def _fmt_train(r: dict[str, Any] | None) -> str:
            if not r:
                return "n/a"
            return (
                f"{r.get('decompose')} val_rmse={r.get('val_rmse')} val_r2={r.get('val_r2')} "
                f"time={r.get('fit_time_sec')} model={r.get('model')}"
            )

        lines.append(f"- best_val_rmse: {_fmt_train(best_val)}")
        lines.append(f"- fastest_fit: {_fmt_train(fastest)}")
        pareto = _pareto_front(ok, x_key="val_rmse", y_key="fit_time_sec")
        if pareto:
            lines.append("- pareto(val_rmse,time): " + ", ".join(str(r.get("decompose")) for r in pareto[:8]))
        if best_val:
            links = _collect_train_plot_links(best_val, base_dir=base_dir)
            if links:
                lines.append("- plots(best_val_rmse): " + ", ".join(links))
        lines.append("")

    path.parent.mkdir(parents=True, exist_ok=True)
    path.write_text("\n".join(lines) + "\n", encoding="utf-8")


def _case_methods(case: str) -> list[str]:
    # Use bench-safe variants where needed.
    gf = "graph_fourier_bench"
    sh = "spherical_harmonics_scipy_bench"
    ss = "spherical_slepian_scipy"
    if case == "rectangle_scalar":
        return ["dct2", "fft2", "fft2_lowpass_k64", "wavelet2d_k64", "pswf2d_tensor_bench", gf, "pod_svd", "pod_em", "rbf_expansion_k64"]
    if case == "rectangle_vector":
        return ["helmholtz", "helmholtz_poisson", gf, "pod_joint_em", "dct2"]
    if case == "disk_scalar":
        return ["zernike", "pseudo_zernike", "fourier_bessel_neumann", "fourier_jacobi", "polar_fft", "disk_slepian_bench", gf, "pod_em", "fft2_disk", "fft2_lowpass_disk_k64"]
    if case == "disk_vector":
        return ["pseudo_zernike", "fourier_bessel_neumann", "polar_fft", gf, "pod_joint_em", "gappy_graph_fourier_bench", "rbf_expansion_k64"]
    if case == "annulus_scalar":
        return ["annular_zernike", "polar_fft", gf, "pod_em", "rbf_expansion_k64"]
    if case == "annulus_vector":
        return ["polar_fft", gf, "pod_joint_em", "gappy_graph_fourier_bench", "rbf_expansion_k64"]
    if case == "arbitrary_mask_scalar":
        return ["gappy_graph_fourier_bench", "rbf_expansion_k64", "pod_em", "wavelet2d_k64", gf]
    if case == "arbitrary_mask_vector":
        return ["gappy_graph_fourier_bench", "rbf_expansion_k64", "pod_joint_em"]
    if case == "sphere_grid_scalar":
        return [sh, ss, gf, "dct2"]
    if case == "sphere_grid_vector":
        return [sh, ss, gf, "dct2"]
    raise ValueError(f"unknown case: {case}")


def main() -> int:
    ap = argparse.ArgumentParser()
    ap.add_argument("--dataset-root", type=str, default="data/benchmarks/v1/offset_noise_36")
    ap.add_argument("--runs-root", type=str, default="runs/benchmarks/v1")
    ap.add_argument("--seed", type=int, default=123)
    ap.add_argument("--stages", type=str, default="decomposition,preprocessing,train")
    ap.add_argument("--cases", type=str, default="")
    ap.add_argument("--continue-on-error", action="store_true", default=True)
    ap.add_argument(
        "--collect-only",
        action="store_true",
        default=False,
        help="Do not run pipelines; just collect existing per-case leaderboards into a global summary.",
    )
    args = ap.parse_args()

    dataset_root = Path(args.dataset_root)
    if not dataset_root.is_absolute():
        dataset_root = PROJECT_ROOT / dataset_root
    runs_root = Path(args.runs_root)
    if not runs_root.is_absolute():
        runs_root = PROJECT_ROOT / runs_root

    stages = [s.strip() for s in str(args.stages).split(",") if s.strip()]
    if not stages:
        raise ValueError("--stages is empty")

    default_cases = [
        "rectangle_scalar",
        "rectangle_vector",
        "disk_scalar",
        "disk_vector",
        "annulus_scalar",
        "annulus_vector",
        "arbitrary_mask_scalar",
        "arbitrary_mask_vector",
        "sphere_grid_scalar",
        "sphere_grid_vector",
    ]
    if args.cases.strip():
        cases = [c.strip() for c in args.cases.split(",") if c.strip()]
    else:
        cases = default_cases

    pipeline_process = None
    if not args.collect_only:
        # Import pipeline lazily (ensures repo root is on sys.path when run from anywhere).
        import sys

        for p in (str(PROJECT_ROOT / "src"), str(PROJECT_ROOT)):
            if p not in sys.path:
                sys.path.insert(0, p)
        from processes import pipeline as pipeline_process  # type: ignore[assignment]

    all_decomp_rows: list[dict[str, Any]] = []
    all_train_rows: list[dict[str, Any]] = []

    for case in cases:
        case_dataset_root = dataset_root / case
        if not (case_dataset_root / "manifest.json").exists():
            raise FileNotFoundError(f"manifest.json not found for case: {case_dataset_root}")

        output_root = runs_root / case
        run_dir = output_root / "pipeline"

        cfg: dict[str, Any] = {
            "seed": int(args.seed),
            "task": {
                "name": "pipeline",
                "stages": stages,
                "decompose_list": _case_methods(case),
                "coeff_post_list": ["none"],
                "model_list": ["ridge"],
                "energy_threshold": 0.9,
                "continue_on_error": bool(args.continue_on_error),
                "decomposition_sort_by": "field_rmse",
                "train_sort_by": "val_rmse",
            },
            "run_dir": str(run_dir),
            "output": {"root": str(output_root), "name": "pipeline"},
            "dataset": {"name": "npy_dir", "root": str(case_dataset_root), "mask_policy": "allow_none"},
            # Placeholder; will be overridden by manifest.json via resolve_domain_cfg.
            "domain": {"name": "rectangle", "x_range": [-1.0, 1.0], "y_range": [-1.0, 1.0]},
            "split": {"name": "all"},
            "preprocess": {"name": "basic"},
            "eval": {"name": "benchmark", "metrics": ["field_rmse", "field_r2", "energy_cumsum"]},
            # Crucial: heterogeneous decomposers (complex/wavelet) need a dispatch codec.
            "codec": {"name": "auto_codec_v1"},
            # Recommended for offset-dominant fields: learn/predict offset separately and decompose residual.
            "offset_split": {"enabled": "auto", "f_offset": 5.0, "max_samples": 128, "seed": int(args.seed)},
            "coeff_post": {"name": "none"},
            "model": {"name": "ridge"},
            "train": {"name": "basic"},
            "viz": {"name": "basic"},
        }

        if not args.collect_only:
            assert pipeline_process is not None
            pipeline_process.main(cfg)

        # Collect per-case leaderboards for a global summary.
        decomp_csv = run_dir / "outputs" / "tables" / "leaderboard_decomposition.csv"
        train_csv = run_dir / "outputs" / "tables" / "leaderboard_train.csv"
        decomp_rows = _read_csv(decomp_csv)
        train_rows = _read_csv(train_csv)
        for r in decomp_rows:
            r["case"] = case
        for r in train_rows:
            r["case"] = case
        all_decomp_rows.extend(decomp_rows)
        all_train_rows.extend(train_rows)

    summary_dir = runs_root / "summary"
    _write_csv(summary_dir / "benchmark_summary_decomposition.csv", all_decomp_rows)
    _write_csv(summary_dir / "benchmark_summary_train.csv", all_train_rows)
    _write_axis_summary_md(
        summary_dir / "benchmark_axes_summary.md",
        decomp_rows=all_decomp_rows,
        train_rows=all_train_rows,
        cases=cases,
    )

    # Lightweight index markdown.
    md = summary_dir / "benchmark_summary.md"
    md.parent.mkdir(parents=True, exist_ok=True)
    md.write_text(
        "\n".join(
            [
                "# Benchmark Summary (v1)",
                "",
                f"- cases: {len(cases)}",
                f"- dataset_root: {dataset_root}",
                f"- runs_root: {runs_root}",
                "",
                "## Outputs",
                f"- decomposition: {summary_dir / 'benchmark_summary_decomposition.csv'}",
                f"- train: {summary_dir / 'benchmark_summary_train.csv'}",
                f"- axes: {summary_dir / 'benchmark_axes_summary.md'}",
                "",
            ]
        )
        + "\n",
        encoding="utf-8",
    )
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
